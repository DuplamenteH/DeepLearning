{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Classificação de roupas Usando DL.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOHEc0E7KWipmPyWfL0tm6s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DuplamenteH/DeepLearning/blob/main/Classifica%C3%A7%C3%A3o_de_roupas_Usando_DL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmbKmLHLvI1N"
      },
      "source": [
        "# INTRODUÇÃO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqBv3CKpsHio"
      },
      "source": [
        "## Neste projeto vamos usar um dataset Fashion MNIST que é bem padrão para estudos como este. O dataset surgiu com intuito de substituir o MNIST q é outro dataset com usado para estudos,porém bem mais fácil de desvendar,pois qualquer CNN conseguiam atingir facilmente a acurácia de 97%, logo não representava mais um desafio para visão computacional.\n",
        "\n",
        "### *Cada imagem pertence exclusivamente a uma única classe. A tabela abaixo segue a documentação do Fashion MNIST, onde são documentados os **10** **labels** possíveis:*\n",
        "\n",
        "<table>\n",
        "  <tbody><tr>\n",
        "    <th>Label</th>\n",
        "    <th>Class</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>0</td>\n",
        "    <td>T-shirt/top</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>1</td>\n",
        "    <td>Trouser</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>2</td>\n",
        "    <td>Pullover</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>3</td>\n",
        "    <td>Dress</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>4</td>\n",
        "    <td>Coat</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>5</td>\n",
        "    <td>Sandal</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>6</td>\n",
        "    <td>Shirt</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>7</td>\n",
        "    <td>Sneaker</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>8</td>\n",
        "    <td>Bag</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>9</td>\n",
        "    <td>Ankle boot</td>\n",
        "  </tr>\n",
        "</tbody></table>\n",
        "\n",
        "<p align=\"center\"><img src=\"https://raw.githubusercontent.com/carlosfab/escola-data-science/master/img/embedding.gif\" height=\"300px\"></p>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Vamos usar o framework tensorflow 2 para este projetos, tanto para carregar os dados quanto para normaliza-los, e criar o modelo.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEIfpXZHvUIx"
      },
      "source": [
        "# Obtendo os dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeG_VOxLvYDj"
      },
      "source": [
        "## Existem duas formas são elas :\n",
        "* [Diretamente a partir do repositório no Github](https://github.com/zalandoresearch/fashion-mnist) .\n",
        "* Carregando a partir do TensorFlow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5sB4yLIr80v"
      },
      "source": [
        "# imports\n",
        "%tensorflow_version 2.x\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib as mpl\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# configurar a visualização\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'svg'\n",
        "mpl.style.use( 'ggplot' )\n",
        "plt.style.use('fivethirtyeight')\n",
        "sns.set(context=\"notebook\", palette=\"dark\", style = 'whitegrid' , color_codes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWhHjd-qMWMd"
      },
      "source": [
        "## Carregando os Dados nas variaeis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0WZPVwEv3VD"
      },
      "source": [
        "# carregando os dados do Fashion MNIST\n",
        "(X_train_orig, y_train_orig),(X_test_orig, y_test_orig) = keras.datasets.fashion_mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwMtISB0MxoX"
      },
      "source": [
        "# Labels de acordo com a documentação ,obs:Estou traduzindo as mesmas.\n",
        "class_name = [ 'Camisetas/Top','Calça', 'Suéter',\n",
        "              'Vestidos', 'Casaco', 'Sandálias',\n",
        "              'Camisas', 'Tênis', 'Bolsas', 'Botas']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdFN21pKNUNk"
      },
      "source": [
        "# Exploração dos Dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLh2sSjVNoUo"
      },
      "source": [
        "### Agora vamos começar a entender nossos dados, como baixamos nossas dados da API do tensorflow não precisamos nos preocupar com dados nulos e esses demais erros.\n",
        "### Para começar vou verificar as dimensões dos nossos arrays q devem ser de 60.000 amostras de treino e 10.000 de teste, sendo cada imagem com 28x28 pixels com apenas com tons de cinza, ou seja , tendo apenas 1 canal."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76YEXHARNY6Y"
      },
      "source": [
        "# ver a dimensionalidade dos DataFrames\n",
        "print(\"Dimensionalidade dos DataFrames:\")\n",
        "print(\"X_train_orig:\", X_train_orig.shape)\n",
        "print(\"y_train_orig:\", y_train_orig.shape)\n",
        "print(\"X_test_orig:\", X_test_orig.shape)\n",
        "print(\"y_test_orig:\", y_test_orig.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prz7s3ZLOuvO"
      },
      "source": [
        "Agora vamos vericar se os datasets (treino e teste) estão com uma proporção idel entre as classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1v0L-tppOcHm"
      },
      "source": [
        "# verificando os val únicos por classes(treino)\n",
        "print(\"y_train_orig:\")\n",
        "np.unique(y_train_orig, return_counts=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lghthttdPS7t"
      },
      "source": [
        "Por último, vamos visualizar algumas imagens com seus respectivos labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUHG4tsfPK2U"
      },
      "source": [
        "plt.figure(figsize=(8,8))\n",
        "\n",
        "for i in range(25):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(X_train_orig[i], cmap=plt.cm.binary)\n",
        "    plt.xlabel(class_name[y_train_orig[i]])\n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9om_n9SLQMz8"
      },
      "source": [
        "# Pré-Processamentos dos Dados\n",
        "\n",
        "Antes de criar o modelo e treinar a rede, vamos noramalizar nossas imagens.\n",
        "\n",
        "## Normalizar os pixels\n",
        "Para normalizar dos pixels, basta dividi-lo pelo valor maximo que um pixel pode atingir , então vamos digidir as imagens por 255.0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGJlIu0KQDU7"
      },
      "source": [
        "\"\"\"\n",
        "    Aqui vamos usar uma função lambda para percorrer \n",
        "    nossas imagens e dividir todas por 255.0\n",
        "\"\"\"\n",
        "f = lambda x :(x/255.0).astype(\"float32\")\n",
        "\n",
        "X_train = f(X_train_orig)\n",
        "X_test = f(X_test_orig)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dy0HFFJzRcKm"
      },
      "source": [
        "## Redimensionar as Imagens\n",
        "\n",
        "Um dos principais problemas ao se trabalhar com dados não-estruturados, é a dimensão desde dados, já que o primeiro layer da nossa CNN espera um único tensor que contenha todos os pixels da imagem, pois o TensorFlow espera uma lista com 4 dim, e não 60000 itens com 28 x 28 x 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuOHetcPRbCh"
      },
      "source": [
        "# redimensionar as imagens\n",
        "X_train = X_train.reshape((X_train.shape[0],28,28,1))\n",
        "X_test = X_test.reshape((X_test.shape[0],28,28,1))\n",
        "\n",
        "\n",
        "print(\"X_train: {}\".format(X_train.shape))\n",
        "print(\"X_test:\\t{}\".format(X_test.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWUIvcW6VfMZ"
      },
      "source": [
        "## One-Hot Encoding\n",
        "Vamos utilizamos para otimizar nossos ganhos com o modelo, pois se não aplicarmos teremos que usar `loss=\"sparse_categorical_crossentropy\"`.Apenas usar essa loss não é problema o problema em que o modelo de deep learning vai da a entender que a existe uma relação entre a ordem dos labels e isso será muito prejudical ao nosso trabalho."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3S-Rsi8TUsQl"
      },
      "source": [
        "y_train = keras.utils.to_categorical(y_train_orig)\n",
        "y_test = keras.utils.to_categorical(y_test_orig)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqSNWAPrYCJs"
      },
      "source": [
        "# Criando nossas Rede Neural."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unWFWWUFYH6X"
      },
      "source": [
        "Existem  ***N*** formar de se contruir uma CNN. Nós vamos utilizar um arquiterua simplificada da VGGNET, que é implementtada em um artigo do ***Adrian Rosebroke dono do site pyimagesearch e também foi modificada por Carlos Melo dono do blog sigmoidal.***\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRhIgNo8ZFwt"
      },
      "source": [
        "***CONV => RELU => CONV=> RELU => POOL***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoMlfpTUYA8e"
      },
      "source": [
        "def buildModel():\n",
        "    model = keras.models.Sequential()\n",
        "    model.add(keras.layers.Conv2D(32, 3, padding=\"same\", activation='relu',))\n",
        "    model.add(keras.layers.BatchNormalization(axis=1))\n",
        "    model.add(keras.layers.Conv2D(32, (3, 3), padding=\"same\", activation='relu'))\n",
        "    model.add(keras.layers.BatchNormalization(axis=1))\n",
        "    model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(keras.layers.Dropout(0.25))\n",
        "\n",
        "    # second CONV => RELU => CONV => RELU => POOL layer set\n",
        "    model.add(keras.layers.Conv2D(64, (3, 3), padding=\"same\", activation='relu'))\n",
        "    model.add(keras.layers.BatchNormalization(axis=1))\n",
        "    model.add(keras.layers.Conv2D(64, (3, 3), padding=\"same\", activation='relu'))\n",
        "    model.add(keras.layers.BatchNormalization(axis=1))\n",
        "    model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(keras.layers.Dropout(0.25))\n",
        "\n",
        "    # first (and only) set of FC => RELU layers\n",
        "    model.add(keras.layers.Flatten())\n",
        "    model.add(keras.layers.Dense(512, activation='relu'))\n",
        "    model.add(keras.layers.BatchNormalization())\n",
        "    model.add(keras.layers.Dropout(0.5))\n",
        "\n",
        "    # softmax classifier\n",
        "    model.add(keras.layers.Dense(10, activation='softmax'))\n",
        "\n",
        "    return model\n",
        "model = buildModel()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKPN6B-deNF9"
      },
      "source": [
        "Como visto acima, nossos dados estão com os labels one-hot encoded, será utilizado `loss=\"categorical_crossentropy\"` para compilar nosso modelo.\n",
        "\n",
        "Vamos também informar que queremos 1/3 de nossos dados para validação"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eg-qgKkZhSY"
      },
      "source": [
        "model.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFBSZR94Z8Xk"
      },
      "source": [
        "Agora vamos treinar o modelo e salvar suas informações do treinamento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3Q9uQF9e3ii"
      },
      "source": [
        "continuar...."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6nFX2SHZ70H"
      },
      "source": [
        "history = model.fit(X_train, y_train,epochs=40, validation_split= 0.3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9MLBCVXeHAd"
      },
      "source": [
        "# Avaliando o Modelo\n",
        "Depois de treinado  nosso modelo atingiu uma acurácia de A nos dados de treino e A nos dados de validação.\n",
        "Porém, se obervamos melhor veremos q ela não teve um desempenho bom para categoria \" Camisas\".\n",
        "Para aumentar essa acuracia podemos usar tecnicas de data augmentation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JG3NZfBaTWb"
      },
      "source": [
        "y_hat = model.predict(X_test)\n",
        "y_hat_class = np.argmax(y_hat, axis=1)\n",
        "print(classification_report(y_test_orig, y_hat_class, target_names=class_name))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yq0hW98dOjxt"
      },
      "source": [
        "Agora vamos observar o grafico do nosso treino."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGWfz28TOoO3"
      },
      "source": [
        "pd.DataFrame(history.history).plot()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wNLQpWMPs8d"
      },
      "source": [
        "Olhando o grafico podemos identicar um pequeno overfitting, mas que n prejudicou tanto o desempenho da nossa CNN perante os dados de validação."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9I5BzQYoP-9w"
      },
      "source": [
        "Agora vamos Colocar nossas dados de teste para ver quão bom nosso modelo está."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSzeyEu2P9XR"
      },
      "source": [
        "score = model.evaluate(X_test, y_test)\n",
        "\n",
        "# verificar o desempenho do modelo\n",
        "print('Loss: {:.4f}'.format(score[0]))\n",
        "print('Acurácia: {:.4f}'.format(score[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFxvw0IjQLbn"
      },
      "source": [
        "Como podemos ver acima nosso modelo esta muito bom, ou seja , ele permanece genérico o suficiente para lidar com dados novos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBifZ1CVQIM5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjTnhfraQXd4"
      },
      "source": [
        "# Melhorias\n",
        "\n",
        "Para melhorar ainda o desempenho, devemos fazer um data augmentation, no nosso dataset de treino, com isso iremos melhorar a acuracia na detecção de \"Camisas\" e \"Camisetas/Top\".\n",
        "-=-----=---=---=---\n",
        "Podemos também mudar o otmizador , e ver se muda algo em nossa acuracia"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iqe2uVJFQzLz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}